{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Joke Generator using LLM Fine-tuning\n",
        "- **Initial Idea/Motivation:**\n",
        "  - Taking a small model like Phi-2 (2.7B) or even TinyLlama (1.1B) and fine-tuning it on a specific genre of jokes within 4-5 lines max (e.g., clever, silly, punny, tech). Then, quantizing it to the highest possible level (e.g., 4-bit or 3-bit GGUF) and running it.\n",
        "  - This is the ultimate test of the (quantization + fine-tuning) combination by creating a fun, usable application with a model under 2GB in size.\n",
        "- **Tech Stack**:\n",
        "    - **Base Model**: Phi-2 (2.7B).\n",
        "    - **Fine-tuning (PEFT+Quatization)**: Implemented QLoRA (Quantized Low-Rank Adaptation), combined with 4-bit quantization, for parameter-efficient training.\n",
        "- **Curated Dataset Approach**:\n",
        "    - **Targeted Creation**: Generated 400 high-quality examples (100 per category) across four safe topics: [\"Technology & Programming\", \"Coffee & Beverages\", \"Food & Cooking\", \"Animals & Pets\"].\n",
        "    - **Generation Method**: Used DeepSeek Chat with carefully crafted prompts to ensure: Consistent Format (4-5 lines), Content Safety (avoiding offensive content), Category Relevance.\n",
        "- **Demo**:\n",
        "    - **HuggingFace Model Repository**: https://huggingface.co/nanditab35/phi-2-jokebot-peft\n",
        "    - **HuggingFace Space Repository**: https://huggingface.co/spaces/nanditab35/jokebot"
      ],
      "metadata": {
        "id": "EiPLkxWpR_nI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for **JokeBot - AI Comedy Generator**"
      ],
      "metadata": {
        "id": "lDeommeQ98US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation & Setup"
      ],
      "metadata": {
        "id": "1dBMlvXYvf4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "\n",
        "#!pip install -qU transformers accelerate peft bitsandbytes datasets trl huggingface_hub\n",
        "!pip install -qU transformers accelerate peft bitsandbytes datasets huggingface_hub"
      ],
      "metadata": {
        "id": "3FO8bRYVH-pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & GPU Check"
      ],
      "metadata": {
        "id": "K0FNZvMpwO9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports and GPU verification\n",
        "# Step 2: Import libraries\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig)\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import Dataset\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 3: Check GPU availability\n",
        "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {
        "id": "PJjRW9H1wRvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CONFIG DICT"
      ],
      "metadata": {
        "id": "2i-EnzpUTcx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG_DICT = {\n",
        "    \"MODEL_NAME\": \"microsoft/phi-2\",\n",
        "    \"JOKE_TYPES_ARR\": [\"tech\", \"coffee\", \"foodie\", \"animals\"]\n",
        "}"
      ],
      "metadata": {
        "id": "k3zvifKTThUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Keys"
      ],
      "metadata": {
        "id": "EIfOMU3t-yK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "HF_API_KEY = userdata.get('HF_API_KEY')\n",
        "login(token=HF_API_KEY)"
      ],
      "metadata": {
        "id": "Fk5oWMca-1MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "aiWcS9tqwZpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare training data\n",
        "with open('jokebot_training_data.json', 'r') as f:\n",
        "    training_data = json.load(f)\n",
        "\n",
        "# Formatting the data\n",
        "def format_instruction(example):\n",
        "    return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}<|endoftext|>\"\n",
        "\n",
        "formatted_data = [format_instruction(example) for example in training_data]\n",
        "train_dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(\"Sample training example:\")\n",
        "print(formatted_data[0])"
      ],
      "metadata": {
        "id": "tqHVmiMswcjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "GUfeT5ibS7-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "model_name = CONFIG_DICT[\"MODEL_NAME\"]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize with truncation and padding\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "    # For causal LM, labels are the same as input_ids\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    # batch_size = 100,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"âœ… Dataset tokenized successfully!\")\n",
        "print(f\"Sample tokenized keys: {list(tokenized_dataset[0].keys())}\")"
      ],
      "metadata": {
        "id": "e3oFQRKLS-Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Setup with QLoRA"
      ],
      "metadata": {
        "id": "1qoJ1DQTwSQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load model with quantization\n",
        "print(\"Loading Phi-2 model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Prepare for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "ouwRwxH8wVlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Enable gradient checkpointing to save memory\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "aoE3I1d9wZAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Setup"
      ],
      "metadata": {
        "id": "veC5s_ZTwdQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # We're doing causal LM, not masked LM\n",
        "    pad_to_multiple_of=8,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./phi-2-jokebot\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    save_total_limit=3,\n",
        "    prediction_loss_only=True,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=\"none\",  # Disable wandb in Colab\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "qyGpKX-2wf0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Execution"
      ],
      "metadata": {
        "id": "Y_-898niwwBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(\"./phi-2-jokebot\")\n",
        "print(\"âœ… Training completed and model saved!\")"
      ],
      "metadata": {
        "id": "4JdsL9A6w0DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Fine-tuned Model"
      ],
      "metadata": {
        "id": "bI01aUwZw2Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model for testing\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"Testing fine-tuned model...\")\n",
        "\n",
        "# Load the saved model\n",
        "model_path = \"./phi-2-jokebot\"\n",
        "\n",
        "# Create text generation pipeline\n",
        "joke_pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_path,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Test with different categories\n",
        "test_prompts = [\n",
        "    \"Generate a short joke within 4-5 lines that is coming from tech topic\",\n",
        "    \"Generate a short joke within 4-5 lines that is coming from food topic\",\n",
        "    \"Generate a short joke within 4-5 lines that is coming from animals topic\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nðŸŽ¯ Test {i+1}: {prompt}\")\n",
        "    result = joke_pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.1\n",
        "    )\n",
        "    print(\"Generated:\")\n",
        "    print(result[0]['generated_text'])\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "f1XBLA1fw4Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge LLM PEFT Model with the Base Model Before GGUF Conversion\n",
        "- avoided this option for simplicity, and directly uploaded the PEFT models to HuggingFace Model Repository"
      ],
      "metadata": {
        "id": "jUFX_aUmJhSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from peft import PeftModel\n",
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# # During DEVELOPMENT - Use PEFT model\n",
        "# def load_peft_model():\n",
        "#     base_model = AutoModelForCausalLM.from_pretrained(CONFIG_DICT[\"MODEL_NAME\"], device_map=\"auto\")\n",
        "#     model = PeftModel.from_pretrained(base_model, \"./phi-2-jokebot\", device_map=\"auto\",)\n",
        "#     return model\n",
        "\n",
        "# # Merge LoRA model with the Base Model - Helps llama.cpp understand BaseModel Architecture\n",
        "# def merge_lora_adapter():\n",
        "#     # Load tokenizer first (very lightweight)\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\n",
        "#         CONFIG_DICT[\"MODEL_NAME\"],\n",
        "#         trust_remote_code=True\n",
        "#     )\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "#     # Load base model with low memory\n",
        "#     base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#         CONFIG_DICT[\"MODEL_NAME\"],\n",
        "#         torch_dtype=torch.float16,\n",
        "#         device_map=\"auto\",\n",
        "#         trust_remote_code=True,\n",
        "#         low_cpu_mem_usage=True  # Critical for low memory\n",
        "#     )\n",
        "#     # Load PEFT model\n",
        "#     model = PeftModel.from_pretrained(base_model, \"./phi-2-jokebot\")\n",
        "#     # merged_model = model.merge_and_unload() # Making the Application Crash on Colab T4 GPU\n",
        "#     # Export both model AND tokenizer\n",
        "#     model.save_pretrained(\n",
        "#         \"./phi-2-jokebot-merged\",\n",
        "#         safe_serialization=True,\n",
        "#         max_shard_size=\"2GB\"\n",
        "#     )\n",
        "#     tokenizer.save_pretrained(\"./phi-2-jokebot-merged\")\n",
        "\n",
        "#     print(\"âœ… LoRA adapter merged!\")\n",
        "\n",
        "# # For DEPLOYMENT - Convert to GGUF once, then use GGUF\n",
        "# # def convert_for_deployment():\n",
        "# #     # Merge only for conversion\n",
        "# #     merged_model = merge_lora_adapter()\n",
        "# #     convert_to_gguf(merged_model)\n",
        "# #     # Then use the GGUF file forever\n",
        "\n",
        "# # In PRODUCTION - Use GGUF\n",
        "# def load_production_model():\n",
        "#     return Llama(model_path=\"./phi-2-jokebot-q4_k_m.gguf\")"
      ],
      "metadata": {
        "id": "xArc67evJh8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge_lora_adapter()"
      ],
      "metadata": {
        "id": "VhlRKhsKMRfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zip the Saved Model files for Download purpose"
      ],
      "metadata": {
        "id": "2XxncaFNyz73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"phi-2-jokebot\", 'zip', \"./phi-2-jokebot\")"
      ],
      "metadata": {
        "id": "SAd2fhB2y5gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Push the PEFT model to HF Space (if HF_API_KEY has Wrrite Permission)\n",
        "- The purpose behind this is - From HF Space use Gradio UI to run the JokeGen Bot\n",
        "- If HF_API_KEY does not have write permission, then upload the model files manually"
      ],
      "metadata": {
        "id": "q5gwRepbw4jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import HfApi\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # Login\n",
        "# from huggingface_hub import login\n",
        "# login(token=HF_API_KEY) # Needs a HF_API_KEY with Write Access\n",
        "\n",
        "# # Upload your PEFT model\n",
        "# api = HfApi()\n",
        "# api.upload_folder(\n",
        "#     folder_path=\"./phi-2-jokebot\",\n",
        "#     # repo_id=\"your-username/phi-2-jokebot-peft\",\n",
        "#     repo_type=\"model\"\n",
        "# )\n",
        "\n",
        "# print(\"Uploaded to HF Hub!\")\n",
        "# print(\"Now you can:\")\n",
        "# print(\"1. Use the HF interface to create GGUF versions\")\n",
        "# print(\"2. Or download and use Spaces with your PEFT model\")\n",
        "# print(\"3. Or use their conversion tools via API\")"
      ],
      "metadata": {
        "id": "t9A60Xz7BO-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzEauP5PyL0D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}